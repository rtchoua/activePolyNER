\section{Conclusion}
\label{sect:apner_conclusion}
The lack of access to large amount of expert-annotated training data impedes the adoption of recent machine learning techniques in certain scientific applications.
%\ian{How is the next reworded sentence?}\roselyne{much improved}
PolyNER overcomes this challenge by % is a generalizable system that 
using
approximate candidate generation and active learning to target 
expert input so that accurate scientific named entity recognition can be performed at low cost.
We show that by using NLP techniques, we can bootstrap a word vector classifier of scientific entities.
Using polyNER's labels and a classifier of character-enhanced word embedding vectors, we achieve 
%achieves 31.7\% precision and 82.3\% recall,  a 
performance comparable to a best-of-breed
%\loganfussingaboutrecallandprecision \roselyne{came with PR curve showing good trade off margins}
hybrid NER model (CDE+) that combines a dictionary, expert-created
rules, and machine learning algorithms.
CDE+ was trained on the CHEMDNER corpus:
a collection of \nistnum{10000} PubMed abstracts with 84,355 chemical entity mentions labeled manually by expert chemistry literature curators, following annotation guidelines specifically defined for this task~\cite{krallinger2015chemdner}. 
%\logan{Very long sentence, break it up.}
In contrast, polyNER was trained on data annotated using just five hours of expert time and minimal untrained crowd input.
%\logan{This is your most important conclusion, perhaps bring it earlier in the conclusion}
%\ian{Is it important to have the phrase ``While we note that CDE+ is intended to extract polymers and their properties,''?}
%While we note that CDE+ is intended to extract polymers and their properties, 
Our work highlights the potential of using minimal amount of data and focused expert input in order to enable machine learning techniques for previously unmined scientific entities. 
We are currently exploring using polyNER-labeled data to annotate text for other NER approaches,
such as bidirectional long short-term memory models.
We also plan to explore the generalizability of our approach further by applying it extracting previously unmined scientific entities in other fields.
%With a view to exploring generalizability, we are also working to apply polyNER
%to quite different problems, such as extracting dataset names from social science
%literature. 
%\logan{Maybe be a little more broad here. "We hope that our findings will ...", etc.}\roselyne{this is also too close to previous conclusion, hadn't worked on it enough yet}