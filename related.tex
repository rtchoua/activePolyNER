\section{Related Work}
\label{sect:relatedwork}
NER and other information extraction tasks rely on large amount of training data, which is expensive to obtain.
Weakly supervised learning methods work with much less training data and aim to address this challenge.
They generally fall under two categories: semi-supervised learning and active learning~\cite{zhou2017brief}.
Semi-supervised learning attemps to automatically label data based on a set of labeled data.
Active learning assumes there is a source of knowledge, such as a human expert, that can be queried to label a selected batch of unlabeled data. 

\subsection{Semi-supervised Approaches}
\textit{Bootstrapping} is a semi-supervised technique, which starts from a small set of seed relation instances and iteratively learns more relation instances and extraction patterns.
Snowball~\cite{agichtein2000snowball} which improved the DIPRE system~\cite{brin1998extracting}, used an intuitive idea to collect new entity relations using a set of seed entity pairs.
The authors start with a handfull of seed tuples of \textit{ORGANIZATION} and \textit{LOCATION}, Snowball attempts to learn the relation \textit{HeadquarteredIn} by assuming that every time the tuples appear in close proximity to each other, the text in between illustrates the desired relation.
This text can then be used to discover new tuples, which can in turn be used as seed for the next discovery iteration.
Of course, organizations may be located but not headquartered in multiple cities, hence it is important to inspect the quality of extraction patters to reduce noise in the generated output.

\textit{Distant supervision} maps known entities and relations from a structured knowledge 
base onto unstructured text~\cite{peters2014machine,zhang2013geodeepdive}. 
With freely available structured knowledge base such as DBPedia~\cite{auer2007dbpedia} and Freebase~\cite{bollacker2008freebase}, it is possible to leverage a large set of known entity pairs to generate training data
%~cite{mintz2009distant}; 
%in this work authors assume that if two entities participate in a relation, any sentence that contains both entities descripes that relation. 
%Because that is not always the case, they extract features from different sentences to define
%lexical, syntactic and named entity tag features. 
%They use standard multi-class logistic regression as the classification algorithm and reach almost 70\% of precision based on human judgment.
For the last decade, probabilistic approaches have been proposed to allow the system to select automatically
For example PaleoDeepDive~\cite{peters2014machine}, built upon DeepDive~\cite{zhang2015deepdive}, automatically extracts
paleontological data from text, tables, and figures in scientific publications. 
For good performance in such applications, such approach often relies on and extends large databases: for example,
PaleoDeepDive uses PaleoDB~\cite{PaleoDB}. 
The system labels any entity pair that appears in the database as \textit{True}.
The user defines features (e.g. entity pairs that are too far apart are marked \textit{False}), the system then uses statistical inference to determine the probability that each newly discovered pair of interest is \textit{True}.

Similarly \textit{Data programming} uses
\textit{labeling functions} (user-defined programs that provide labels for subsets of data)~\cite{ratner2016data}. 
Errors due to differences in accuracy and conflicts between labeling functions are 
addressed by learning and modeling the accuracies of the labeling functions. 
Under certain conditions, data programming achieves results on par with those of supervised learning methods.
While writing concise scripts to define rules may seem to be a more reasonable task for annotators 
than exhaustively annotating text, it still requires expert guidance.  
In data programming as in boostrapping and distant supervision it is important to evaluate the quality of functions and extraction patters to decrease noisy patterns.

\subsection{Active Learning}
Active learning~\cite{zhou2017brief} assumes that the gold standard labels of unlabeled instances can be queried from an oracle (domain expert or source of knowledge). 
The goal of active learning is to decrease the cost of labeling by requesting a limited number of labels from the oracle, that have been deemed most valuable by the learner.
Uncertainty sampling approaches define ``valuable'' data by measuring uncertainty in the predictions.
For example, in the case of a single learner, querying predictions with maximum entropy in which the learner assigns all classes with equal probability~\cite{lewis1994heterogeneous} or predictions closest to the decision boundary in the case of support vector machine classifiers~\cite{campbell2000query}.
In the case of multiple learners, query-by-committee requests labels for unlabeled instances on which the learners disagree the most~\cite{seung1992query}.
Uncertainty sampling and query-by-committee are representative approaches based on informativeness, where informativeness measure show well an unlabeled instance helps reduce the uncertainty. 
Another selection criterion addresses representativeness, which measures how well an instance helps represent the structure of input patterns; in this case selection is made by querying data from unlabeled clusters of data~\cite{nguyen2004active,dasgupta2008hierarchical}.

\subsection{PolyNER}
In this work, we implement a low-cost scientific fact extraction system. 
We ask: how can we quickly generate annotated data for scientific named entity recognition?
We face two challenges: (1) lack of (expensive) training data in some fields, including our own polymer science application which lacks free access to large polymer databases; and (2) the need for domain expert curators, which impedes the use of crowdsourcing platforms such as Amazon Turk~\cite{buhrmester2011amazon} or Figure8~(\url{https://www.figure-eight.com/}).
For these reasons, our approach is similar to bootstrapping but applied to named entities rather than entity pairs. 
We generate approximate labels by extracting targets similar to a few seed entities using word representations and vector similarity measures.
To address noise in the output, expert label small batches of candidates, which can be used to train a word vector classifier and propose final scientific named entities.
Eventually, we will explore using polyNER along with distant supervision and data programming to extract polymer properties.
An example rule could be defined as: \textit{if a sentence contains a polymer name and the words ``glass transition'', then extract number(s) in the sentence as potential glass transition temperature(s) for that polymer.}
We discuss the architecture of our system in more details in the next section.




 


