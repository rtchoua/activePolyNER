\section{Related Work}
\label{sect:apner_related}
NER and other information extraction tasks rely on a large amount of training data, which are expensive to obtain.
Weakly supervised learning methods work with much less training data and aim to address this challenge.
They generally fall under two categories: semi-supervised learning and active learning~\cite{zhou2017brief}.
The key difference between the two is that the former relies on approximately labeled data (as opposed to correctly labeled data for supervised learning) and the latter starts off with unlabeled data.
Semi-supervised learning attempts to label data automatically by using prior knowledge and a set of labeled data. 
For example, it assumes that if $x$ and $y$ are similar, they probably have the same label (first cluster the whole dataset, then label each cluster with labeled data~\cite{zhu2005semi}).
%\ian{The reference to ``cluster and label algorithms'' is obscure to me.}\roselyne{Does the sentence still make sense to you without that ``hint'', the idea is to cluster the data and assign }
%\kyle{Could be a clearer description of semi-supervised learning}\logan{Agreed. The key thing that separates it from other learning is that it uses labeled and unlabeled data}\roselyne{Added key difference explicitely as suggested}
Active learning assumes there is a source of knowledge, such as a human expert, that can be queried 
to label a selected batch of unlabeled data. 
%\logan{It sounds like these two categories are mutually exclusive. Do you think so? Can you state what differentiates them explicitly?}\roselyne{I think maybe one key element in polyNER is that we combine the two, we start of with approximate labels (distance labels), then continue with active learning) but I don't think I am the only one doing this, have found other examples scheming papers, like clustering and doing active learning across previously clustered data}

\subsection{Semi-supervised Approaches}
\textit{Bootstrapping} is a semi-supervised technique, which starts from a small set of seed relation instances and iteratively learns more relation instances and extraction patterns.
Snowball~\cite{agichtein2000snowball} which improved the DIPRE system~\cite{brin1998extracting}, used an intuitive idea to collect new entity relations using a set of seed entity pairs.
In the DIPRE system, the intuitive assumption is that, given a few seed entity relations, the text between two known target entities in close proximity of each other describes and constitutes a \textit{pattern} of the relation between the two. 
Since that is not the case in practice, the system uses a limited set of regular expressions to limit useful patterns, hence decreasing the number of false positives.
A key improvement of Snowball
is that its patterns include named entity tags (PERSON, LOCATION, ORGANIZATION, etc.).
Given a handful of seed tuples of ORGANIZATION and LOCATION, Snowball attempts to learn the relation \textit{HeadquarteredIn} by assuming that each time the tuples appear in close proximity to each other, 
the text in between illustrates the desired relation.
This text can then be used to discover new tuples, which can in turn be used as seeds for the next discovery round.
Of course, organizations may be located but not headquartered in multiple cities;
hence it is important to inspect the quality of extraction patterns to reduce noise in the generated output.

\textit{Distant supervision} maps known entities and relations from a structured knowledge 
base onto unstructured text~\cite{peters2014machine,de2016deepdive}. 
With freely available structured knowledge base such as DBPedia~\cite{auer2007dbpedia} and Freebase~\cite{bollacker2008freebase}, it is possible to leverage a large set of known entity pairs to generate training data.
%~cite{mintz2009distant}; 
%in this work authors assume that if two entities participate in a relation, any sentence that contains both entities descripes that relation. 
%Because that is not always the case, they extract features from different sentences to define
%lexical, syntactic and named entity tag features. 
%They use standard multi-class logistic regression as the classification algorithm and reach almost 70\% of precision based on human judgment.
Over the past decade, probabilistic approaches have been proposed to allow automatic selection.
For example, PaleoDeepDive~\cite{peters2014machine}, built upon DeepDive~\cite{de2016deepdive}, automatically extracts
paleontological data from text, tables, and figures in scientific publications. 
For good performance, such approaches often rely on and extend large databases: for example,
PaleoDeepDive uses PaleoDB~\cite{PaleoDB}. 
The system labels any entity pair that appears in the database as \textit{True}.
%\kyle{Following sentence is broken (and missing a bracket}
The user defines features (e.g. if a specific keyword appears between two entities, that pair a certain attribute is labeled \textit{True}, but if the entity pairs are too far apart, another attribute is marked \textit{False}), 
%\kyle{I don't get how this is an example feature. Seems like its more labels.}), \roselyne{Hope it's clarified, they're more like user defined functions and rules - this is Snorkel's ancestor}
the system then uses statistical inference to determine the probability that each newly discovered pair of interest is \textit{True}.

\textit{Data programming}, as used in the Snorkel system~\cite{ratner2016data}, has users define 
\textit{labeling functions} to provide labels for data subsets. 
Errors due to differences in accuracy and conflicts between labeling functions are 
addressed by learning and modeling the accuracies of the labeling functions. 
Under certain conditions, data programming achieves results on par with those of supervised learning methods.
While writing concise scripts to define rules may seem to be a more reasonable task for annotators 
than exhaustively annotating text, it still requires expert guidance.  
In data programming as in bootstrapping and distant supervision it is important to evaluate the quality of functions and extraction patterns to decrease noisy patterns.

\subsection{Active Learning}\label{sec:active}
Active learning~\cite{zhou2017brief} assumes that gold standard labels for unlabeled instances can be obtained by
querying an oracle (domain expert or source of knowledge).
The goal of active learning is to decrease labeling costs by requesting a limited number of labels from the oracle, that have been deemed most valuable by the learner.
Uncertainty sampling approaches define ``valuable'' data by measuring uncertainty in the predictions.
For example, in the case of a single learner, querying predictions with maximum entropy in which the learner assigns all classes with equal probability~\cite{lewis1994heterogeneous} or predictions closest to the decision boundary in the case of support vector machine classifiers~\cite{campbell2000query}.
In the case of multiple learners, query-by-committee requests labels for unlabeled instances on which the learners disagree the most~\cite{seung1992query}.
Uncertainty sampling and query-by-committee are representative approaches based on informativeness, where informativeness measures how well an unlabeled instance helps reduce the uncertainty. 
Another selection criterion addresses representativeness, which measures how well an instance helps represent the structure of input patterns; in this case selection is made by querying data from unlabeled clusters of data~\cite{nguyen2004active,dasgupta2008hierarchical}.

\subsection{CDE and CDE+}\label{sec:cde}
We introduce briefly ChemDataExtractor (CDE),
a state-of-the-art chemical NLP tool that combines a dictionary, expert-created
rules, and machine learning algorithms.
CDE was trained on the CHEMDNER corpus:
a collection of \num{10000} PubMed abstracts with \num{84355} chemical entity mentions labeled manually by expert chemistry literature curators, following annotation guidelines specifically defined for this task~\cite{krallinger2015chemdner}.
In previous work, we modified CDE with
manually defined polymer identification rules~\cite{tchoua2017towards}, creating what we term here CDE+.
We compare our methods against both CDE and CDE+ in Section~\ref{sect:apner_results}.

\subsection{Placing polyNER in Context}
%\kyle{Much of this text is repeated in the following section. I wonder if we should delete here and move
%anything that is here now, but not in the next section, into the next section.}\roselyne{I think I like that we are spelling out how polyNER compares to what has been said in related work, I will see if I can shorten it so that it's not a lot of repetition but just comparing to previous work.}
%\ian{I feel that it is clearer to have a subsection here. Also: I think the goal is to put polyNER in context, but I am not sure that it does so clearly, yet. It seems to me to be currently a rather hard-to-follow statement of the polyNER approach, which I would expect to find elsewhere.}
%\roselyne{This was a subsection, but was removed as some found it strange here, but the purpose was indeed to relate polyNER to the previous paragraphs, giving it a second read.}
%\roselyne{The main purpose also was to indicate that there was already work on polyNER and we are extending the work here}
%PolyNER extends previous work to create a low-cost scientific fact extraction system. 
%%We ask: how can we quickly generate annotated data for scientific named entity recognition?
%Specifically, we address two challenges: 
%(1) lack of (expensive) training data in some fields, including our own polymer science application which lacks free access to large polymer databases; and 
%(2) the need for domain expert curators, which impedes the use of crowdsourcing platforms such as Amazon Mechanical Turk~\cite{buhrmester2011amazon} or Figure8~(\url{https://www.figure-eight.com/}).
%\ian{I find it odd that we are introducing the challenges that polyNER addresses here. It seems that we have
%already done that, to some extent, in the introduction and the background. Ideally that would be in one place.}
%\roselyne{Come back to this, after rereading intro and intro to architecture. This was supposed to clearly separate polyNER form the other systems. I think this section can start with sentence below. Would be nice to put text above in intro if it fits, it states clearly why we designed polyNER}
%\ian{The text alternates between third person and first person. Maybe be consistent third person: ``The polyNER approach ..." not ``our approach"?}
PolyNER has in common with prior work that it combines semi-supervised and active learning~\cite{nguyen2004active,basu2004active}.
%\ian{In the intro, the three stage are named candidate generation, labeling, and classification. Perhaps use those terms here too?}
The initial sampling of unlabeled data is similar to bootstrapping but applied to named entities rather than entity pairs. 
%\logan{Confusing. If PolyNER is similar to bootstrapping, why are are you describing it in the active learning section? I'm generally not sure why this paragraph is in "Related Work"}\roselyne{This was its own section, related polyner to all previous sections including bootstrapping and not just active learning, but felt a bit weird to Kyle, so we moved it to motivation, perhaps it fits before current changes to architecture to motivate what we are doing now. Will just try to do this link between our work and related work in each section and have a polyNER paragraph in architecture.}
%\ian{I want to confirm that the next sentence is correct. It states that you generate approximate labels by extracting
%words similar to seed entities, implying (to me) that this process somehow generates labels (automatically?). I thought this was the
%candidate generation phase.}
The initial batch of labels contains strings deemed \textit{similar} to a few seed entities, 
with similarity determined by using word representations and vector distance measures.
As this approach is approximate and is likely to include errors 
(words that have similar context to entities but are not actual entities), the next phase is expert labeling. 
Subsequent batches of labels are obtained via active learning and used 
%To address noise in the output, expert label small batches of candidates, 
%which can be used\ian{the term ``can be used'' confuses me. Is the training that is defined here part of your system?}
 to train a context-aware word vector classifier in the last phase.
%\ian{what does ``final'' mean?}\roselyne{the idea here is that candidates from the candidate generation phase are loosely labeled polymers, but subsequently corrected by experts, the real assignments are done at the classification. Will rephrase.}
%scientific named entities.
%\kyle{Somewhat hesitant to include the next sentences. Doesnt feel like it belongs here}\roselyne{I think it's ok here if we're trying to emphasize how it is different, but I guess you want to phrase it as a contribution and it may need to be repeated in the intro/conclusion. Making a note for tomorrow}
PolyNER can be used in a way that complements other scientific NER approaches. 
For example, it could be used as a scientific entity tagger (i.e., recognizer) to be used with data programming to extract polymer properties.
%PolyNER could provide a scientific entity tagger (i.e., recognizer) to be used with data programming to extract polymer properties.\ian{I am not sure what this sentence is here for. I think that you are trying to explain how polyNER can 
%complement other approaches. If so, then how about a paragraph that speaks to that directly. E.g., ``PolyNER can be used in a way that complements other scientific NER approaches. For example, it could be used as a scientific entity tagger (i.e., recognizer) to be used with data programming to extract polymer properties.}
%An example rule could be defined as: \textit{if a sentence contains a polymer name and the words ``glass transition,'' then extract number(s) in the sentence as potential glass transition temperature(s) for that polymer.}
%We discuss the architecture of our system in more details in the next section.
